{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This document contains three main parts:\n",
    "1. Package essentials\n",
    "2. Examples\n",
    "3. Codes for future work\n",
    "\n",
    "To run the examples, please execute the package essential first.\n",
    "\n",
    "The package essentials load necessary libraries and define all essential functions required to fullfill the functionalities of the Bayesreg package.\n",
    "\n",
    "Examples use the diabetes data from sklearn dataset. Different models and priors are executed and compared with the original Matlab version of the package.\n",
    "\n",
    "Code for future work contains functions not fully supported for now, thus are commented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package essentials\n",
    "\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import datetime\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNpArray(X, y, varnames):\n",
    "    n, p = X.shape\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if varnames is None and list(X.columns) != list(range(X.shape[1])):\n",
    "            varnames = np.array(X.columns).reshape(-1,1)\n",
    "        X = np.array(X)\n",
    "    elif isinstance(X, list):\n",
    "        X = np.array(X)\n",
    "        \n",
    "    if isinstance(y, pd.Series):\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        \n",
    "    return X, y, varnames\n",
    "\n",
    "\n",
    "# function sample beta0 from the posterior distribution\n",
    "# validated\n",
    "def sample_beta0(X, z, mu_z, Xt1, b, sigma2, omega2):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        X      - [n x p]\n",
    "        z      - [n x 1]\n",
    "        b      - [p x 1]\n",
    "        mu_z   - float/None\n",
    "        Xt1    - [p x 1]/None\n",
    "        sigma2 - float\n",
    "        omega2 - [n x 1]\n",
    "    \n",
    "    Return values:\n",
    "        b0     - float\n",
    "        m      - float\n",
    "    '''\n",
    "    if mu_z is not None:\n",
    "        n = z.shape[0]\n",
    "        if Xt1 is None:\n",
    "            m = mu_z\n",
    "        else:\n",
    "            m = (mu_z - np.matmul(b.T, Xt1)[0][0]/n)\n",
    "        v = sigma2/n\n",
    "    else:\n",
    "        W = np.sum(1/omega2)\n",
    "        m = np.sum((z - np.matmul(X, b))/omega2) / W\n",
    "        v = sigma2 / W\n",
    "    \n",
    "    b0 = m + np.random.normal()*np.sqrt(v)\n",
    "    return b0, m\n",
    "\n",
    "# function generate random variables from the inverse normal distribution\n",
    "# validated\n",
    "def randinvg(mu, lamda):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        mu    - [n x 1]\n",
    "        lamda - float\n",
    "    \n",
    "    Return values:\n",
    "        out   - [n x 1]\n",
    "    '''\n",
    "    lamda = 1/lamda\n",
    "    n = mu.shape[0]\n",
    "    V = np.random.normal(size = (n, 1))**2\n",
    "    \n",
    "    out = mu + 0.5*mu/lamda*(mu*V - np.sqrt(4*mu*lamda*V+mu**2*V**2))\n",
    "    \n",
    "    l = (np.random.uniform(size = (n, 1)) >= mu/(mu + out))\n",
    "    out[l] = mu[l]**2/out[l]\n",
    "    \n",
    "    out = np.max(np.concatenate((out, np.array([[10**-5] for i in range(n)])), axis=1), axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# function generate random variables from exponential distribution\n",
    "# validated\n",
    "def exprnd_fast(mu):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        mu - [n x 1]/float\n",
    "        \n",
    "    Return values:\n",
    "        r  - [n x 1]\n",
    "    '''\n",
    "    if not isinstance(mu, np.ndarray):\n",
    "        n = 1\n",
    "    else:\n",
    "        n = mu.shape[0]\n",
    "    r = -mu * np.log(np.random.uniform(size = (n, 1)))\n",
    "    return r\n",
    "\n",
    "# function which standardise the design matrix X and target y if required\n",
    "# validated\n",
    "def standardise(X, y):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        X     - [n x p]\n",
    "        y     - [n x 1]/None\n",
    "    \n",
    "    Return values:\n",
    "        X     - [n x p]\n",
    "        meanX - [1 x p]\n",
    "        stdX  - [1 x p]\n",
    "        y     - [n x 1]\n",
    "        meany - float\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    meanX = np.mean(X, axis=0).reshape(1, -1)\n",
    "    stdX = (np.std(X, axis=0)*np.sqrt(n)).reshape(1, -1)\n",
    "    \n",
    "    X = X - meanX\n",
    "    X = X/stdX\n",
    "    \n",
    "    if y is not None:\n",
    "        meany = np.mean(y)\n",
    "        y = y - meany\n",
    "        return X, meanX, stdX, y, meany\n",
    "    else:\n",
    "        return X, meanX, stdX\n",
    "    \n",
    "# function which reverse the standardisation process\n",
    "# validated\n",
    "def unstandardise(X, muX, normX, y, muy):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        X     - [n x p]\n",
    "        muX   - [1 x p]\n",
    "        normX - [1 x p]\n",
    "        y     - None/[n x 1]\n",
    "        muy   - None/float\n",
    "    \n",
    "    Return values:\n",
    "        X     - [n x p]\n",
    "        y     - [n x 1]\n",
    "    '''\n",
    "    X = X*normX + muX\n",
    "    if y is not None:\n",
    "        y = y + muy\n",
    "    return X, y\n",
    "\n",
    "# function forms the diagonal entries of the regularization matrix\n",
    "# validated\n",
    "def make_Lambda(sigma2, tau2, lambda2):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        sigma2     - float\n",
    "        tau2       - float\n",
    "        lambda2    - [p x 1]\n",
    "    \n",
    "    Return values:\n",
    "        Lambda     - [p x 1]\n",
    "        delta2prod - [p x 1]\n",
    "    '''\n",
    "    delta2prod = np.ones(shape=(lambda2.shape[0], 1))\n",
    "    Lambda = sigma2 * tau2 * lambda2 * delta2prod\n",
    "    return Lambda, delta2prod\n",
    "\n",
    "# function sample beta from the posterior distribution\n",
    "# validated\n",
    "def sample_beta(X, z, mvnrue, b0, sigma2, tau2, lambda2, delta2prod, omega2, XtX, Xty, Xt1, weights, gprior, b):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        X          - [n x p]\n",
    "        z          - [n x 1]\n",
    "        mvnrue     - bool\n",
    "        b0         - float\n",
    "        sigma2     - float\n",
    "        tau2       - float\n",
    "        lambda2    - [p x 1]\n",
    "        delta2prod - [p x 1]\n",
    "        omega2     - [n x 1]\n",
    "        XtX        - [p x p]\n",
    "        Xty        - [p x 1]\n",
    "        Xt1        - [p x 1]\n",
    "        weights    - bool\n",
    "        gprior     - bool\n",
    "        b          - [p x 1]\n",
    "        \n",
    "    Return values:\n",
    "        b          - [p x 1]\n",
    "        muB        - [p x 1]\n",
    "    '''\n",
    "    sigma = np.sqrt(sigma2)\n",
    "    Lambda = sigma2 * tau2 * lambda2 * delta2prod\n",
    "    \n",
    "    # no block sampling at this time\n",
    "    if weights or Xty is None or not mvnrue:\n",
    "        alpha = z - b0\n",
    "        \n",
    "    if mvnrue:\n",
    "        if Xty is None or weights:\n",
    "            omega = np.sqrt(omega2)\n",
    "            X = X/omega\n",
    "            b, muB = fastmvg_rue(X, None, alpha, None, Lambda, sigma2, omega, gprior, XtX)\n",
    "        else:\n",
    "            if Xt1 is None:\n",
    "                b, muB = fastmvg_rue(None, XtX, None, Xty, Lambda, sigma2, None, gprior, XtX)\n",
    "            else:\n",
    "                b, muB = fastmvg_rue(None, XtX, None, Xty - b0*Xt1, Lambda, sigma2, None, gprior, XtX)\n",
    "    else:\n",
    "        omega = np.sqrt(omega2)\n",
    "        X = X/omega\n",
    "        b, muB = fastmvg_bhat(X, alpha, Lambda, sigma, omega)\n",
    "        \n",
    "    return b, muB\n",
    "\n",
    "# sampler for multivariate gaussian\n",
    "# validated\n",
    "def fastmvg_rue(Phi, PtP, alpha, Ptalpha, D, sigma2, omega, gprior, XtX):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        Phi - [n x p]\n",
    "        PtP - [p x p]\n",
    "        alpha - [n x 1]\n",
    "        Ptalpha - [p x 1]\n",
    "        D - [p x 1]\n",
    "        sigma2 - 1 x 1\n",
    "        omega - [n x 1]\n",
    "        gprior - 1 x 1\n",
    "        XtX - [p x p]\n",
    "        \n",
    "    Return values:\n",
    "        x - [p x 1]\n",
    "        m - [p x 1]\n",
    "    '''\n",
    "    if PtP is None:\n",
    "        PtP = np.matmul(Phi.T, Phi)\n",
    "        \n",
    "    if Ptalpha is None:\n",
    "        Ptalpha = np.matmul(Phi.T, (alpha/omega))\n",
    "        \n",
    "    p = D.shape[0]\n",
    "    \n",
    "    if not gprior:\n",
    "        L = np.linalg.cholesky(PtP/sigma2 + np.diag(1/D.reshape(-1)))\n",
    "    else:\n",
    "        L = np.linalg.cholesky(PtP/sigma2 + XtX/D[0])\n",
    "    \n",
    "    v = np.linalg.solve(L, Ptalpha/sigma2)\n",
    "    m = np.linalg.solve(L.T, v)\n",
    "    w = np.linalg.solve(L.T, np.random.normal(size = (p, 1)))\n",
    "    x = m + w\n",
    "    return x, m\n",
    "\n",
    "\n",
    "# sampler for multivariate gaussian\n",
    "# validated\n",
    "def fastmvg_bhat(Phi, alpha, D, sigma, omega):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        Phi - [n x p]\n",
    "        alpha - [n x 1]\n",
    "        D - [p x 1]\n",
    "        sigma - float\n",
    "        omega - [n x 1]\n",
    "        \n",
    "    Return values:\n",
    "        x - [p x 1]\n",
    "        u - [p x 1]\n",
    "    '''\n",
    "    n, p = Phi.shape\n",
    "    u = np.random.normal(size = (p, 1)) * np.sqrt(D)\n",
    "    delta = np.random.normal(size = (n, 1))\n",
    "    \n",
    "    v = np.matmul(Phi, u)/sigma + delta\n",
    "    Dpt = Phi.T*D/sigma\n",
    "    W = np.matmul(Phi, Dpt)/sigma + np.eye(n)\n",
    "    w = np.linalg.solve(W, (alpha/omega/sigma - v))\n",
    "    x = u + np.matmul(Dpt, w)\n",
    "    u = x\n",
    "    return x, u\n",
    "\n",
    "\n",
    "# function sample error variance for linear regression model\n",
    "# validated\n",
    "def sample_sigma2(mu, y, b, ete, omega2, tau2, lambda2, delta2prod, gprior):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        mu         - [n x 1]\n",
    "        y          - [n x 1]\n",
    "        b          - [p x 1]\n",
    "        ete        - float\n",
    "        omega2     - [n x 1]\n",
    "        tau2       - float\n",
    "        lambda2    - [p x 1]\n",
    "        delta2prod - [p x 1]\n",
    "        gprior     - bool\n",
    "        \n",
    "    Return values:\n",
    "        sigma2     - float\n",
    "        muSigma2   - float\n",
    "        e          - [n x 1]\n",
    "    '''\n",
    "    n = y.shape[0]\n",
    "    p = b.shape[0]\n",
    "    \n",
    "    e = None\n",
    "    if ete is None:\n",
    "        e = y - mu\n",
    "        ete = np.sum(e**2/omega2)\n",
    "    \n",
    "    shape = (n+p)/2\n",
    "    \n",
    "    if not gprior:\n",
    "        scale = ete/2 + np.sum(b**2/lambda2/delta2prod)/2/tau2\n",
    "    else:\n",
    "        bXtXb = np.matmul(mu.T, mu)[0][0]\n",
    "        scale = np.sum(e**2/omega2)/2 + bXtXb/tau2/2\n",
    "    sigma2 = scale/np.random.gamma(shape)\n",
    "    muSigma2 = scale/(shape-1)\n",
    "    return sigma2, muSigma2, e\n",
    "\n",
    "\n",
    "# function sample global variance hyperparameter\n",
    "# validated\n",
    "def sample_tau2(b, sigma2, lambda2, delta2prod, xi, mu, gprior, tau_a):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        b: p x 1\n",
    "        sigma2: float\n",
    "        lambda2: p x 1\n",
    "        delta2prod: p x 1\n",
    "        xi: float\n",
    "        mu: n x 1\n",
    "        gprior: bool\n",
    "        tau_a: float\n",
    "        \n",
    "    Return values:\n",
    "        tau2 - float\n",
    "        muTau2 - float\n",
    "    '''\n",
    "    p = b.shape[0]\n",
    "    shape = p/2 + tau_a\n",
    "    \n",
    "    if not gprior:\n",
    "        scale = 1/xi + np.sum(b**2/lambda2/delta2prod)/2/sigma2\n",
    "    else:\n",
    "        scale = 1/xi + np.matmul(mu.T, mu)[0][0]/2/sigma2\n",
    "    tau2 = scale/ np.random.gamma(shape)\n",
    "    muTau2 = scale/(shape-1)\n",
    "    return tau2, muTau2\n",
    "\n",
    "\n",
    "# function sample xi for all models\n",
    "# validated\n",
    "def sample_xi(tau2, tau_ab):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        tau2 - float\n",
    "        tau_ab - float\n",
    "        \n",
    "    Return values:\n",
    "        xi - float\n",
    "    '''\n",
    "    scale = 1 + 1/tau2\n",
    "    if tau_ab == 1:\n",
    "        xi = 1/exprnd_fast(1/scale)\n",
    "    else:\n",
    "        shape = tau_ab\n",
    "        xi = scale/np.random.gamma(shape)\n",
    "    return xi\n",
    "\n",
    "\n",
    "# sample lambda2 for LASSO\n",
    "# validated\n",
    "def sample_lambda2_lasso(b, sigma2, tau2, delta2prod):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        b - [p x 1]\n",
    "        sigma2 - float\n",
    "        tau2 - float\n",
    "        delta2prod - [p x 1]\n",
    "    \n",
    "    Return values:\n",
    "        lambda2 - [p x 1]\n",
    "    '''\n",
    "    mu = np.sqrt(2*tau2*sigma2*delta2prod/b**2)\n",
    "    shape = 2\n",
    "    lambda2 = 1/randinvg(mu, 1/shape)\n",
    "    return lambda2\n",
    "\n",
    "\n",
    "# function sample the lambda2 for horseshoe prior\n",
    "# validated\n",
    "def sample_lambda2_hs(b, sigma2, tau2, nu, delta2prod):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        b - [p x 1]\n",
    "        sigma2 - float\n",
    "        tau2 - float\n",
    "        nu - [p x 1]\n",
    "        delta2prod - [p x 1]\n",
    "        \n",
    "    Return values:\n",
    "        lambda2 - [p x 1]\n",
    "    '''\n",
    "    scale = 1/nu + b**2/2/tau2/sigma2/delta2prod\n",
    "    lambda2 = 1/exprnd_fast(1/scale)\n",
    "    return lambda2\n",
    "\n",
    "\n",
    "# function sample nu for horseshoe prior\n",
    "# validated\n",
    "def sample_nu_hs(lambda2):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        lambda2 - [p x 1]\n",
    "    \n",
    "    Return values:\n",
    "        nu - [p x 1]\n",
    "    '''\n",
    "    scale = 1 + 1/lambda2\n",
    "    nu = 1/exprnd_fast(1/scale)\n",
    "    return nu\n",
    "\n",
    "\n",
    "# function computes the probability of data for the linear models\n",
    "# validated\n",
    "def br_regnlike_mu(error, mu, e, y, s2, tdof):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        error: error model, gaussian, laplace, t\n",
    "        mu: n x 1\n",
    "        e: n x 1\n",
    "        y: n x 1\n",
    "        s2: float\n",
    "        tdof: int\n",
    "        \n",
    "    Return values:\n",
    "        neglike - float\n",
    "        neglogprob - [n x 1]\n",
    "        prob - [n x 1]\n",
    "    '''\n",
    "    \n",
    "    if error != 'binomial':\n",
    "        if error == 'gaussian':\n",
    "            neglogprob = e**2/s2/2 + (1/2)*np.log(2*np.pi*s2)\n",
    "        elif error == 'laplace':\n",
    "            scale = np.sqrt(s2/2)\n",
    "            neglogprob = np.abs(e)/scale + np.log(2*scale)\n",
    "        elif error == 't':\n",
    "            nu = tdof\n",
    "            neglogprob = -sp.special.gammaln((nu+1)/2) + sp.special.gammaln(nu/2) + (nu+1)/2*np.log(1+1/nu*e**2/s2) + np.log(np.pi*nu*s2)/2\n",
    "            \n",
    "    prob = np.exp(-neglogprob)\n",
    "    neglike = np.sum(neglogprob)\n",
    "    \n",
    "    return neglike, neglogprob, prob\n",
    "\n",
    "\n",
    "# function compute the probability of data for the regression models\n",
    "# validated\n",
    "def br_regnlike(error, X, y, beta, beta0, s2, tdof):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        error - string\n",
    "        X - [n x p]\n",
    "        y - [n x 1]\n",
    "        beta - [p x nsamples]\n",
    "        beta0 - [1 x namples]\n",
    "        s2 - float\n",
    "        tdof - float\n",
    "        \n",
    "    Return values:\n",
    "        neglike - [nsamples x 1]\n",
    "        neglogprob - [n x nsamples]\n",
    "        prob - [n x nsamples]\n",
    "        mu - [n x nsamples]\n",
    "    '''\n",
    "    mu = np.matmul(X, beta) + beta0\n",
    "    if error != 'binomial':\n",
    "        e = mu - y\n",
    "        if error == 'gaussian':\n",
    "            neglogprob = e**2/s2/2 + (1/2)*np.log(2*np.pi*s2)\n",
    "        elif error == 'laplace':\n",
    "            scale = np.sqrt(s2/2)\n",
    "            neglogprob = np.abs(e)/scale + np.log(2*scale)\n",
    "        elif error == 't':\n",
    "            nu = tdof\n",
    "            neglogprob = -sp.special.gammaln((nu+1)/2) + sp.special.gammaln(nu/2) + (nu+1)/2*np.log(1+1/nu*e**2/s2) + np.log(np.pi*nu*s2)/2\n",
    "            \n",
    "    prob = np.exp(-neglogprob)\n",
    "    neglike = np.sum(neglogprob, axis=0)\n",
    "    \n",
    "    return neglike, neglogprob, prob, mu\n",
    "\n",
    "\n",
    "# function compute the model stats\n",
    "# validated\n",
    "def br_compute_model_stats(y, X, retval):\n",
    "    # model type\n",
    "    gaussian = False\n",
    "    lapalce = False\n",
    "    tdist = False\n",
    "    binomial = False\n",
    "\n",
    "    if retval['runstats']['model'] in ['binomial','logistic']:\n",
    "        binomial = True\n",
    "        model = 'binomial'\n",
    "    elif retval['runstats']['model'] in ['gaussian','normal']:\n",
    "        gaussian = True\n",
    "        model = 'gaussian'\n",
    "    elif retval['runstats']['model'] in ['laplace', 'l1']:\n",
    "        laplace = True\n",
    "        model = 'laplace'\n",
    "    elif retval['runstats']['model'] in ['t','student']:\n",
    "        tdist = True\n",
    "        model = 't'\n",
    "        \n",
    "    # stats for continuous model\n",
    "    if not binomial:\n",
    "        mu = np.matmul(X, retval['muB']) + retval['muB0']\n",
    "        modelstats = {}\n",
    "        \n",
    "        modelstats['logl'] = -br_regnlike(model, X, y, retval['muB'], retval['muB0'], retval['muSigma2'], retval['runstats']['tdof'])[0]\n",
    "        modelstats['r2'] = 1 - np.sum((y - mu)**2) / np.sum((y - np.mean(y))**2)\n",
    "        \n",
    "    return modelstats\n",
    "\n",
    "\n",
    "# function calculate the effective sample size\n",
    "# validated\n",
    "def ess(x):\n",
    "    n = len(x)\n",
    "    s = min(n-1, 2000)\n",
    "    g = my_autocorr(x, s)\n",
    "    \n",
    "    G = g[1:s] + g[2:s+1]\n",
    "    ix = [i for i, x in enumerate((G < 0).reshape(-1)) if x]\n",
    "    \n",
    "    ESS = 0\n",
    "    ESSfrac = 0\n",
    "    if len(ix) > 0:\n",
    "        k = ix[0]\n",
    "        \n",
    "        V = g[0] + 2*np.sum(g[1:k+1])\n",
    "        ACT = V/g[0]\n",
    "        ESS = min(n/ACT, n)\n",
    "        ESSfrac = ESS/n\n",
    "    return ESS, ESSfrac\n",
    "\n",
    "# function calculate autocorrelation\n",
    "# validated\n",
    "def my_autocorr(y, order):\n",
    "    y = y - np.mean(y)\n",
    "    nFFT = int(2**(nextpow2(len(y)) + 1))\n",
    "    F = np.fft.fft(y, nFFT, axis=0)\n",
    "    F = F * np.conj(F)\n",
    "    acf = np.fft.ifft(F, axis=0)\n",
    "    \n",
    "    acf = acf[:order+1]\n",
    "    acf = np.real(acf)\n",
    "    acf = acf/acf[0]\n",
    "    return acf\n",
    "\n",
    "\n",
    "# function get the smallest number 2 to the power of greater than n\n",
    "# validated\n",
    "def nextpow2(n):\n",
    "    n = abs(n)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.ceil(np.log2(n))\n",
    "    \n",
    "    \n",
    "# function calculate the percentile\n",
    "# validated\n",
    "def prctile(X, perct):\n",
    "    '''\n",
    "    X: n x p\n",
    "    perct: numeric\n",
    "    '''\n",
    "    N, _ = X.shape\n",
    "    X_sort = np.sort(X, axis = 0)\n",
    "    \n",
    "    perct_array = np.array([100*((0.5 + n)/N) for n in range(N)])\n",
    "    if perct in perct_array:\n",
    "        return X_sort[perct_array == perct][0].reshape(-1, 1)\n",
    "    else:\n",
    "        if perct < perct_array[0]:\n",
    "            return X_sort[0][0].reshape(-1, 1)\n",
    "        elif perct > perct_array[-1]:\n",
    "            return X_sort[-1][0].reshape(-1, 1)\n",
    "        else:\n",
    "            X_low = X_sort[perct_array < perct][-1]\n",
    "            X_up = X_sort[perct_array > perct][0]\n",
    "            p_low = perct_array[perct_array < perct][-1]\n",
    "            p_up = perct_array[perct_array > perct][0]\n",
    "            return (X_low + ((perct - p_low)/(p_up - p_low))*(X_up - X_low)).reshape(-1, 1)\n",
    "        \n",
    "\n",
    "# function get the ranking of variables\n",
    "# validated\n",
    "def bfr(b):\n",
    "    '''\n",
    "    Inputs:\n",
    "        b: [p x nsamples] Regression parameters\n",
    "    \n",
    "    Return:\n",
    "        varranks: [p x 1]\n",
    "    '''\n",
    "    p, nsamples = b.shape\n",
    "\n",
    "    ranks = np.zeros(shape = (p, nsamples))\n",
    "    \n",
    "    for i in range(nsamples):\n",
    "        value = np.abs(b[:,i])\n",
    "        O = [x for _, x in sorted(zip(value, list(range(len(value)))), reverse=True)]\n",
    "        ranks[O, i] = range(1, p+1)\n",
    "        \n",
    "    q = prctile(ranks.T, 75)\n",
    "    O = [x for _, x in sorted(zip(q, list(range(len(q)))))]\n",
    "\n",
    "    varranks = np.array([[None] for i in range(p+1)])\n",
    "    j = 1\n",
    "    k = 1\n",
    "    \n",
    "    for i in range(p):\n",
    "        if i >= 1:\n",
    "            if q[O[i]] != q[O[i-1]]:\n",
    "                j += k\n",
    "                k = 1\n",
    "            else:\n",
    "                k += 1\n",
    "        varranks[O[i]] = j\n",
    "\n",
    "    return varranks\n",
    "\n",
    "\n",
    "def sample_omega2_laplace(e, sigma2):\n",
    "    mu = np.sqrt(2*sigma2/e**2)\n",
    "    lamda = 2\n",
    "    omega2 = 1/randinvg(mu, 1/lamda)\n",
    "    return omega2\n",
    "\n",
    "def sample_omega2_tdist(e, sigma2, tdof):\n",
    "    n = len(e)\n",
    "    a = (tdof + 1)/2\n",
    "    b = e**2/sigma2/2 + tdof/2\n",
    "    omega2 = b/np.random.gamma(shape = a, size = (n, 1))\n",
    "    return omega2\n",
    "\n",
    "\n",
    "def br_summary(beta, beta0, retval):\n",
    "    varnames = retval['Xstats']['varnames']\n",
    "    nx = retval['Xstats']['nx']\n",
    "    px = retval['Xstats']['px']\n",
    "    \n",
    "    model = retval['runstats']['model']\n",
    "    prior = retval['runstats']['prior']\n",
    "    nsamples = retval['runstats']['nsamples']\n",
    "    burnin = retval['runstats']['burnin']\n",
    "    thin = retval['runstats']['thin']\n",
    "    normalize = retval['runstats']['normalize']\n",
    "    runBFR = retval['runstats']['runBFR']\n",
    "    sortrank = retval['runstats']['sortrank']\n",
    "    displayor = retval['runstats']['displayor']\n",
    "    tdof = retval['runstats']['tdof']\n",
    "    #isVarCat = retval['runstats']['']\n",
    "    #XtoZ = retval['runstats']['']\n",
    "    \n",
    "    # Model type\n",
    "    gaussian = False\n",
    "    lapalce = False\n",
    "    tdist = False\n",
    "    binomial = False\n",
    "\n",
    "    if model in ['binomial','logistic']:\n",
    "        binomial = True\n",
    "        model = 'binomial'\n",
    "    elif model in ['gaussian','normal']:\n",
    "        gaussian = True\n",
    "        model = 'gaussian'\n",
    "    elif model in ['laplace', 'l1']:\n",
    "        laplace = True\n",
    "        model = 'laplace'\n",
    "    elif model in ['t','student']:\n",
    "        tdist = True\n",
    "        model = 't'\n",
    "        \n",
    "    # compute ess for each variable\n",
    "    ESSfrac = np.zeros(shape = (px, 1))\n",
    "    for j in range(px):\n",
    "        _, ESSfrac[j] = ess(beta[j,:].reshape(-1, 1))\n",
    "    \n",
    "    # table symbols\n",
    "    chline = '-'\n",
    "    cvline = '|'\n",
    "    cTT    = '+'\n",
    "    \n",
    "    # find length of the longest variable name\n",
    "    maxlen = 12\n",
    "    for i in range(px):\n",
    "        if len(varnames[i]) > maxlen:\n",
    "            maxlen = len(varnames[i])\n",
    "    \n",
    "    fmtstr = ' ' * maxlen\n",
    "    \n",
    "    # display pretable information\n",
    "    if binomial:\n",
    "        modeltxt = 'logistic'\n",
    "    else:\n",
    "        modeltxt = 'linear'\n",
    "    \n",
    "    excess_s = ' ' * (maxlen - 12)\n",
    "    print('Bayesian ' + modeltxt + ' ' + prior + ' regression', '\\n\\n')\n",
    "    print('Number of obs   =' + excess_s + str(nx))\n",
    "    print('Number of vars  =' + excess_s + str(px),'\\n')\n",
    "    \n",
    "    if not binomial:\n",
    "        s2 = retval['muSigma2'][0][0]\n",
    "        if tdist:\n",
    "            s2 = tdof/(tdof-2)*s2\n",
    "        print('MCMC Samples    =' + excess_s + str(nsamples))\n",
    "        print('MCMC Burnin     =' + excess_s + str(burnin))\n",
    "        print('MCMC Thining    =' + excess_s + str(thin), '\\n')\n",
    "        \n",
    "        print('Root MSE        =' + excess_s + str(np.round(np.sqrt(s2), 3)))\n",
    "        print('R-Squared       =' + excess_s + str(np.round(retval['modelstats']['r2'], 3)))\n",
    "        print('WAIC            =' + excess_s + str(np.round(retval['modelstats']['waic'], 3)), '\\n')\n",
    "        \n",
    "    # print table header\n",
    "    print(chline*(maxlen+1), cTT, chline*83)\n",
    "    print(' '*(maxlen - 10) + 'Parameter'+' '*6+'mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS')\n",
    "    print(chline*(maxlen+1), cTT, chline*83)\n",
    "    \n",
    "    # variable information\n",
    "    if runBFR and sortrank:\n",
    "        indices = [x for _, x in sorted(zip(retval['varranks'], list(range(len(retval['varranks'])))))]\n",
    "    else:\n",
    "        indices = list(range(px+1))\n",
    "        \n",
    "    incat = -1\n",
    "    for i in range(px+1):\n",
    "        k = indices[i]\n",
    "        \n",
    "        # regression variables\n",
    "        if k < px:\n",
    "            kappa = retval['tStats'][k]\n",
    "            s = beta[k,:]\n",
    "            mu = retval['muB'][k][0]\n",
    "            if binomial:\n",
    "                mu = retval['medB'][k]\n",
    "        \n",
    "        # intercept\n",
    "        elif k == px:\n",
    "            s = beta0\n",
    "            mu = np.mean(s)\n",
    "            if binomial:\n",
    "                mu = retval['medB0']\n",
    "        \n",
    "        # compute the credible intervals\n",
    "        std_err = np.std(s)\n",
    "        s = s.reshape(-1,1)\n",
    "        qlin = np.concatenate((prctile(s, 2.5).reshape(1, -1), prctile(s, 25).reshape(1, -1), prctile(s, 75).reshape(1, -1), prctile(s, 97.5).reshape(1, -1)), axis=0)\n",
    "        qlog = np.concatenate((prctile(np.exp(s), 2.5).reshape(1, -1), prctile(np.exp(s), 25).reshape(1, -1), prctile(np.exp(s), 75).reshape(1, -1), prctile(np.exp(s), 97.5).reshape(1, -1)), axis=0)\n",
    "        \n",
    "        q = qlin\n",
    "        if binomial and displayor:\n",
    "            mu = np.exp(mu)\n",
    "            std_err=  (qlog[-1] - qlog[0])/2/1.96\n",
    "            q = qlog\n",
    "        \n",
    "        #display results\n",
    "        if k >= px:\n",
    "            tstats = '        .'\n",
    "            E = '        .'\n",
    "        else:\n",
    "            tstats = '    '+str(np.round(kappa[0], 3))\n",
    "            E = '        ' + str(np.round(ESSfrac[k][0]*100, 3))\n",
    "            \n",
    "        if retval['varranks'][k] is None:\n",
    "            rank = '        .'\n",
    "        else:\n",
    "            rank = '    ' + str(retval['varranks'][k][0])\n",
    "        \n",
    "        varname = varnames[k][0]\n",
    "        print(maxlen*' ' + varname + cvline +'    ' +str(np.round(mu, 3))+'    '*2+str(np.round(std_err, 3))+'    '*3+str(np.round(q[0][0], 3))+'    '+str(np.round(q[-1][0], 3))+tstats+rank+E)\n",
    "        \n",
    "        \n",
    "def bayesreg(X, y, model, prior, normalize = True, runBFR = True, nsamples = 1000, burnin = 1000, thin = 5, display = True, \n",
    "             displayor = False, varnames = None, sortrank = False, tdof = 5, catvars = None, nogrouping = False, \n",
    "             groups = None, tau2prior = [0.5,0.5], blocksample = None, blocksize = None, waic = True):\n",
    "    '''\n",
    "    Input parameters:\n",
    "        Required:\n",
    "            X           - [n x p], design matrix\n",
    "\n",
    "            y           - [n x 1], target vector\n",
    "\n",
    "            model       - str, population distribution\n",
    "\n",
    "            prior       - str, prior distribution\n",
    "            \n",
    "        Optional:\n",
    "            normalize   - bool, whether to normalize design matrix X, default True\n",
    "            \n",
    "            runBFR      - bool, whether to compute ranking of importance for predictors, default: True\n",
    "            \n",
    "            nsamples    - int, number of posterior samples, default: 1000\n",
    "\n",
    "            burnin      - int, number of burnin samples, default: 1000\n",
    "\n",
    "            thin        - int, level of thining, default: 5\n",
    "\n",
    "            display     - bool, whether to display summary stats, default: True\n",
    "\n",
    "            displayor   - bool, whether to display odds ratio for logistic regression, default: False\n",
    "\n",
    "            varnames    - [p x 1]/None, names for the variables, default: None\n",
    "\n",
    "            sortrank    - bool, whether to display variables as the order of ranking, default: False\n",
    "\n",
    "            tdof        - int, degree of freedom for student-t distribution, default: 5\n",
    "\n",
    "            carvars     - [ncat x 1]/None indexes of categorical variables, default: None\n",
    "\n",
    "            nogrouping  - bool, whether stop automatically grouping categorical variables after expansion, default: False\n",
    "\n",
    "            groups      - None/2D list with the first dimension indicating groups and the second dimension indicating which predictors are in the group, default: None\n",
    "\n",
    "            tau2prior   - str/[a, b], hyperparameters of beta prior on shrinkage parameters, default: [0.5, 0.5]\n",
    "\n",
    "            blocksample - None/int, the number of blocks when sampling beta, default: None\n",
    "\n",
    "            blocksize   - None/int, the approximate size of each block when sampling beta, default: None\n",
    "\n",
    "            waic        - bool, whether to compute WAIC, default: True\n",
    "    \n",
    "    Return parameters:\n",
    "            beta        - [p x nsamples], posterior samples of beta\n",
    "            beta0       - [1 x nsamples], posterior samples of beta0\n",
    "            retval      - dict, additional sampling information\n",
    "    '''\n",
    "    start = datetime.datetime.now()\n",
    "    # format X, y to numpy array\n",
    "    X, y, varnames = toNpArray(X, y, varnames)\n",
    "    \n",
    "    # Data dimension\n",
    "    nx, px = X.shape\n",
    "    ny, py = y.shape\n",
    "    \n",
    "    # Constants\n",
    "    MAX_PRECOMPUTED_PX = 2e4\n",
    "    \n",
    "    #expectedModel= ['gaussian', 'normal', 'laplace', 't', 'studentt', 'binomial', 'logistic']\n",
    "    #expectedPrior = ['ridge','rr','horseshoe','hs','lasso','hs+','horseshoe+','gprior','g']\n",
    "    #expectedTau2Prior = ['hc','sb','uniform']\n",
    "    \n",
    "    # Model type\n",
    "    gaussian = False\n",
    "    laplace = False\n",
    "    tdist = False\n",
    "    binomial = False\n",
    "    \n",
    "    if model in [\"binomial\", \"logistic\"]:\n",
    "        binomial = True\n",
    "        model = \"binomial\"\n",
    "    elif model in [\"gaussian\", \"normal\"]:\n",
    "        gaussian = True\n",
    "        model = \"gaussian\"\n",
    "    elif model in [\"laplace\", \"l1\"]:\n",
    "        laplace = True\n",
    "        model = \"laplace\"\n",
    "    elif model in [\"t\", \"student\"]:\n",
    "        tdist = True\n",
    "        model = \"t\"\n",
    "        \n",
    "    # Prior type\n",
    "    gprior = False\n",
    "    ridge = False\n",
    "    lasso = False\n",
    "    horseshoe = False\n",
    "    horseshoeplus = False\n",
    "    \n",
    "    if prior in [\"gprior\", \"g\"]:\n",
    "        gprior = True\n",
    "        prior = 'g'\n",
    "        nogrouping = True\n",
    "    elif prior in [\"ridge\", \"rr\"]:\n",
    "        ridge = True\n",
    "        prior = 'ridge'\n",
    "        nogrouping = True\n",
    "    elif prior in [\"lasso\"]:\n",
    "        lasso = True\n",
    "        prior = 'lasso'\n",
    "    elif prior in [\"horseshoe\", \"hs\"]:\n",
    "        horseshoe = True\n",
    "        prior = 'horseshoe'\n",
    "    elif prior in [\"horseshoe+\", \"hs+\"]:\n",
    "        horseshoeplus = True\n",
    "        prior = 'horseshoe+'\n",
    "        \n",
    "    # type of tau2 prior\n",
    "    if isinstance(tau2prior, str):\n",
    "        if tau2prior == 'hc':\n",
    "            tau_a = 0.5\n",
    "            tau_b = 0.5\n",
    "        elif tau2prior == 'sb':\n",
    "            tau_a = 0.5\n",
    "            tau_b = 1.0\n",
    "        elif tau2prior == 'uniform':\n",
    "            tau_a = 1.0\n",
    "            tau_b = 1.0\n",
    "    else:\n",
    "        tau_a = tau2prior[0]\n",
    "        tau_b = tau2prior[1]\n",
    "    \n",
    "    # create temporary variable names\n",
    "    if varnames is None:\n",
    "        varnames = np.array([['v'+str(i+1)] for i in range(px)])\n",
    "        varnames = np.concatenate((varnames, [['cons']]), axis=0)\n",
    "        \n",
    "    # setup variable processing rule\n",
    "    vars_ = {}\n",
    "    vars_['description'] = 'Variable information'\n",
    "    vars_['varnames'] = varnames\n",
    "    vars_['isVarCat'] = np.array([[False] for i in range(px)])\n",
    "    vars_['isVarCat'][catvars] = True\n",
    "    \n",
    "    \n",
    "    # change y to z   \n",
    "    z = y\n",
    "    weights = laplace or tdist or binomial\n",
    "    \n",
    "    # normalize data\n",
    "    if not normalize:\n",
    "        muX = np.zeros(shape=(1, px))\n",
    "        normX = np.ones(shape=(1, px))\n",
    "    else:\n",
    "        X, muX, normX = standardise(X, None)\n",
    "        \n",
    "    # return values\n",
    "    retval = {\"sparsify_method\":\"\", 'sparseB0':None, \"sparseB\": None, \"medB0\": None, \"medB\": None,\n",
    "              \"muB0\": 0, \"muB\": np.zeros(shape=(px, 1)), \"tau2\": np.zeros(shape=(1, nsamples)), \"xi\": np.zeros(shape=(1, nsamples))}\n",
    "    beta0 = np.zeros(shape = (1, nsamples))\n",
    "    beta = np.zeros(shape = (px, nsamples))\n",
    "    \n",
    "    if not binomial:\n",
    "        retval[\"sigma2\"] = np.zeros(shape=(1, nsamples))\n",
    "        retval[\"muSigma2\"] = 0\n",
    "    if not (ridge or gprior):\n",
    "        retval[\"lambda2\"] = np.zeros(shape = (px, nsamples))\n",
    "    \n",
    "    # Initial values for sampling\n",
    "    b = np.random.normal(size=(px, 1))\n",
    "    sigma2 = 1\n",
    "    e = None\n",
    "    tau2 = 1\n",
    "    xi = 1\n",
    "    lambda2 = np.ones(shape=(px, 1))\n",
    "    omega2 = np.ones(shape=(nx, 1))\n",
    "    nu = np.ones(shape=(px, 1))\n",
    "    phi2 = np.ones(shape=(px, 1))\n",
    "    zeta = np.ones(shape=(px, 1))\n",
    "    XtX = None\n",
    "    Xty = None\n",
    "    Xt1 = None\n",
    "    negll = np.zeros(shape=(1, nsamples))\n",
    "    waicProb = np.zeros(shape=(nx, 1))\n",
    "    waicLProb = np.zeros(shape=(nx, 1))\n",
    "    waicLProb2 = np.zeros(shape=(nx, 1))\n",
    "    \n",
    "    # determine sampling algorithm\n",
    "    mvnrue = True\n",
    "    if(px/nx >= 2):\n",
    "        mvnrue = False;\n",
    "    \n",
    "    # precomputation\n",
    "    precompute = False\n",
    "    if ((gaussian and mvnrue) or gprior) and px < MAX_PRECOMPUTED_PX:\n",
    "        precompute = True\n",
    "        if gaussian:\n",
    "            yty = np.matmul(z.T, z)[0][0]\n",
    "            Xty = np.matmul(X.T, z)\n",
    "        XtX = np.matmul(X.T, X)\n",
    "    \n",
    "    #Always precompute mean(z) if Gaussian\n",
    "    mu_z = None\n",
    "    if gaussian:\n",
    "        mu_z = np.mean(z)\n",
    "        if not normalize:\n",
    "            Xt1 = np.sum(X, axis=0).reshape(-1,1)\n",
    "\n",
    "    # Statistics for result structure retval\n",
    "    retval[\"runstats\"] = {}\n",
    "    retval[\"runstats\"][\"description\"] = \"run arguments\"\n",
    "    retval['runstats']['model']= model\n",
    "    retval['runstats']['prior'] = prior\n",
    "    retval['runstats']['nsamples'] = nsamples\n",
    "    retval['runstats']['burnin'] = burnin\n",
    "    retval['runstats']['thin'] = thin\n",
    "    retval['runstats']['normalize'] = normalize\n",
    "    retval['runstats']['runBFR']= runBFR\n",
    "    retval['runstats']['sortrank'] = sortrank\n",
    "    retval['runstats']['displayor'] = displayor\n",
    "    retval['runstats']['blocksample'] = blocksize\n",
    "    retval['runstats']['tdof'] = tdof\n",
    "    retval['runstats']['tau2prior'] = [tau_a, tau_b]\n",
    "    \n",
    "    if not tdist:\n",
    "        retval['runstats']['tdof'] = None\n",
    "        \n",
    "    # X stats\n",
    "    retval[\"Xstats\"] = {}\n",
    "    retval['Xstats']['description'] = 'Predictor matrix statistics'\n",
    "    retval['Xstats']['varnames'] = varnames\n",
    "    retval['Xstats']['nx'] = nx\n",
    "    retval['Xstats']['px'] = px\n",
    "    retval['Xstats']['muX'] = muX\n",
    "    retval['Xstats']['normX'] = normX\n",
    "    \n",
    "    ######################################\n",
    "    k = -1\n",
    "    iters = 0\n",
    "    while k < nsamples-1:\n",
    "        # sample beta0\n",
    "        b0, muB0 = sample_beta0(X, z, mu_z, Xt1, b, sigma2, omega2)\n",
    "        \n",
    "        # form diagonal Lambda matrix\n",
    "        _, delta2prod = make_Lambda(sigma2, tau2, lambda2)\n",
    "        \n",
    "        # sample beta\n",
    "        b, muB = sample_beta(X, z, mvnrue, b0, sigma2, tau2, lambda2, delta2prod, omega2, XtX, Xty, Xt1, weights, gprior, b)\n",
    "        \n",
    "        mu = None\n",
    "        if gprior or XtX is None or nx < px or waic:\n",
    "            mu = np.matmul(X, b) + b0\n",
    "        \n",
    "        # sample sigma2\n",
    "        if not binomial:\n",
    "            ete = None\n",
    "            if mu is None:\n",
    "                if Xt1 is None:\n",
    "                    ete = yty - 2*np.matmul(Xty.T, b) + np.matmul(np.matmul(b.T, XtX), b) + b0**2*nx - 2*mu_z*nx*b0\n",
    "                else:\n",
    "                    ete = yty - 2*np.matmul(Xty.T, b) + np.matmul(np.matmul(np.concatenate((b, np.array([[b0]]))).T, np.concatenate((np.concatenate((XtX, Xt1)), np.concatenate((Xt1.T, np.array([[nx]])))), axis=1)), np.concatenate(b, np.array([[b0]]))) - 2*mu_z*nx*b0\n",
    "            sigma2, muSigma2, e = sample_sigma2(mu, y, b, ete, omega2, tau2, lambda2, delta2prod, gprior)\n",
    "        \n",
    "        # sample omega2\n",
    "        if weights:\n",
    "            if laplace:\n",
    "                omega2 = sample_omega2_laplace(e, sigma2)\n",
    "            elif tdist:\n",
    "                omega2 = sample_omega2_tdist(e, sigma2, tdof)\n",
    "        \n",
    "        # sample tau2\n",
    "        tau2, _ = sample_tau2(b, sigma2, lambda2, delta2prod, xi, mu, gprior, tau_a)\n",
    "        \n",
    "        # sample xi\n",
    "        xi = sample_xi(tau2, tau_a + tau_b)\n",
    "        \n",
    "        # individual shrinkage\n",
    "        if lasso:\n",
    "            lambda2 = sample_lambda2_lasso(b, sigma2, tau2, delta2prod)\n",
    "        elif horseshoe:\n",
    "            lambda2 = sample_lambda2_hs(b, sigma2, tau2, nu, delta2prod)\n",
    "            nu = sample_nu_hs(lambda2)\n",
    "        elif horseshoeplus:\n",
    "            lambda2 = sample_lambda2_hs(b, sigma2, tau2, nu, delta2prod*phi2)\n",
    "            nu = sample_nu_hs(lambda2)\n",
    "            \n",
    "            phi2 = sample_lambda2_hs(b, sigma2, tau2, zeta, delta2prod*lambda2)\n",
    "            zeta = sample_nu_hs(phi2)\n",
    "            \n",
    "            lambda2 = lambda2 * phi2\n",
    "            \n",
    "        # collect samples\n",
    "        iters += 1\n",
    "        if iters > burnin:\n",
    "            # thining\n",
    "            if iters%thin == 0:\n",
    "                k += 1\n",
    "                # store posterior samples\n",
    "                beta0[0][k] = b0\n",
    "                beta[:,k] = b.T\n",
    "                \n",
    "                # store posterior means\n",
    "                retval[\"muB\"] += muB\n",
    "                retval[\"muB0\"] += muB0\n",
    "                retval[\"tau2\"][0][k] = tau2\n",
    "                \n",
    "                # negloglikelihood of the model\n",
    "                if mu is not None:\n",
    "                    negll[0][k], lprob, prob = br_regnlike_mu(model, mu, e, y, sigma2, tdof)\n",
    "                else:\n",
    "                    negll[0][k] = (nx/2)*np.log(2*np.pi*sigma2) + ete/2/sigma2\n",
    "                    prob = 0\n",
    "                    lprob = 0\n",
    "                \n",
    "                # calculate WAIC\n",
    "                waicProb = waicProb + prob\n",
    "                waicLProb = waicLProb + lprob\n",
    "                waicLProb2 = waicLProb2 + lprob**2\n",
    "                \n",
    "                if not binomial:\n",
    "                    retval['sigma2'][0][k] = sigma2\n",
    "                    retval['muSigma2'] = retval['muSigma2'] + muSigma2\n",
    "                    \n",
    "                if not (ridge or gprior):\n",
    "                    retval['lambda2'][:,k] = lambda2.reshape(-1)\n",
    "                    \n",
    "    # compute average posterior means\n",
    "    retval[\"muB\"] /= nsamples\n",
    "    retval[\"muB0\"] /= nsamples\n",
    "    if not binomial:\n",
    "        retval['muSigma2'] /= nsamples\n",
    "        \n",
    "    # other stats\n",
    "    retval['tStats'] = retval['muB']/np.std(beta, axis=1).reshape(-1,1)\n",
    "    retval['varranks'] = np.array([[None] for i in range(px+1)])\n",
    "    retval['vars'] = vars_\n",
    "\n",
    "    # if required, compute variable ranking\n",
    "    if runBFR:\n",
    "        retval['varranks'] = bfr(beta)\n",
    "        \n",
    "    # compute model fit stats\n",
    "    retval['modelstats'] = br_compute_model_stats(y, X, retval)\n",
    "    retval['modelstats']['negll'] = negll\n",
    "    if waic:\n",
    "        retval['modelstats']['waic_dof'] = np.sum(waicLProb2/nsamples) - np.sum((waicLProb/nsamples)**2)\n",
    "        retval['modelstats']['waic'] = -np.sum(np.log(waicProb/nsamples)) + retval['modelstats']['waic_dof']\n",
    "    else:\n",
    "        retval['modelstats']['waic_dof'] = np.Inf\n",
    "        retval['modelstats']['waic'] = np.Inf\n",
    "        \n",
    "    # rescale coefficients\n",
    "    if normalize:\n",
    "        beta = beta/normX.reshape(-1,1)\n",
    "        beta0 = beta0 - np.matmul(muX, beta)\n",
    "        \n",
    "        retval['muB'] = retval['muB']/normX.reshape(-1,1)\n",
    "        retval['muB0'] = retval['muB0'] - np.matmul(muX, retval['muB'])\n",
    "    \n",
    "    # posterior median estimation\n",
    "    retval['medB'] = np.median(beta, axis=1)\n",
    "    retval['medB0'] = np.median(beta0)\n",
    "    \n",
    "    \n",
    "    retval['runstats']['rundate'] = str(datetime.datetime.now().date())\n",
    "    retval['runstats']['runtime'] = (datetime.datetime.now() - start).total_seconds()\n",
    "    \n",
    "    if display:\n",
    "        br_summary(beta, beta0, retval)\n",
    "        \n",
    "    return retval, beta0, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "Below will show examples trying different models and priors, including gaussian, laplace and student-t for models and ridge, lasso, horseshoe and horseshoe+ for priors. The results are compared with the Matlab version of the Bayesreg.\n",
    "\n",
    "From the comparison it can be observed that the python implementation and the Matlab implementation provide similar results. The R^2 and the WAIC are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442, 1)\n"
     ]
    }
   ],
   "source": [
    "data = sklearn.datasets.load_diabetes(return_X_y=False)\n",
    "X = data['data']\n",
    "print(X.shape)\n",
    "y = data['target'].reshape(-1, 1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian model with ridge prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear ridge regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =54.437\n",
      "R-Squared       =0.515\n",
      "WAIC            =2398.05 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -3.803        59.934            -121.861    117.04    -0.063    10        83.692\n",
      "            v2|    -224.719        60.333            -347.105    -108.006    -3.725    4        100.0\n",
      "            v3|    511.268        66.187            381.382    643.289    7.725    1        100.0\n",
      "            v4|    313.809        64.723            183.672    432.562    4.848    3        100.0\n",
      "            v5|    -184.269        207.842            -629.666    218.705    -0.887    5        100.0\n",
      "            v6|    -1.673        175.645            -314.261    376.091    -0.01    8        100.0\n",
      "            v7|    -157.129        127.854            -389.859    105.468    -1.229    6        100.0\n",
      "            v8|    115.92        130.083            -141.016    383.508    0.891    6        100.0\n",
      "            v9|    505.332        106.774            314.725    732.963    4.733    1        92.333\n",
      "            v10|    77.129        65.15            -46.363    207.523    1.184    8        86.291\n",
      "            cons|    152.279        2.481            147.752    157.293        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'gaussian', 'ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/gaussian_ridge.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian model with Lasso prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear lasso regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =54.388\n",
      "R-Squared       =0.515\n",
      "WAIC            =2397.489 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -3.721        53.14            -106.694    106.726    -0.07    10        94.562\n",
      "            v2|    -211.592        58.917            -315.766    -91.198    -3.591    4        100.0\n",
      "            v3|    523.057        67.461            388.693    657.372    7.754    1        100.0\n",
      "            v4|    306.715        65.803            177.866    434.296    4.661    3        100.0\n",
      "            v5|    -181.111        187.153            -616.076    105.056    -0.968    5        100.0\n",
      "            v6|    3.473        155.315            -266.909    368.097    0.022    7        100.0\n",
      "            v7|    -152.783        116.667            -365.401    73.385    -1.31    5        100.0\n",
      "            v8|    98.577        116.823            -106.284    355.059    0.844    7        91.294\n",
      "            v9|    521.005        106.121            320.29    752.273    4.91    1        100.0\n",
      "            v10|    63.915        59.712            -55.997    177.724    1.07    7        100.0\n",
      "            cons|    152.057        2.633            147.057    156.961        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'gaussian', 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/gaussian_lasso.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian model with horseshoe prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear horseshoe regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =54.424\n",
      "R-Squared       =0.514\n",
      "WAIC            =2397.745 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -2.592        43.056            -94.144    85.588    -0.06    10        87.228\n",
      "            v2|    -195.514        67.562            -320.113    -58.575    -2.894    4        86.766\n",
      "            v3|    534.607        69.647            399.423    666.466    7.676    1        100.0\n",
      "            v4|    300.124        66.688            170.937    433.867    4.5    3        89.813\n",
      "            v5|    -174.993        195.432            -709.323    65.382    -0.895    5        46.762\n",
      "            v6|    15.213        150.865            -222.646    455.361    0.101    7        58.403\n",
      "            v7|    -152.802        120.919            -373.897    68.809    -1.264    5        53.925\n",
      "            v8|    69.588        107.801            -101.674    319.108    0.646    7        65.996\n",
      "            v9|    540.715        109.172            347.275    791.728    4.953    1        86.506\n",
      "            v10|    43.982        55.739            -49.157    169.458    0.789    7        89.234\n",
      "            cons|    152.112        2.543            147.158    157.021        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'gaussian', 'horseshoe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/gaussian_horseshoe.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian model with horseshoe+ prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear horseshoe+ regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =54.422\n",
      "R-Squared       =0.514\n",
      "WAIC            =2397.768 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -1.965        35.44            -77.477    78.471    -0.055    10        100.0\n",
      "            v2|    -193.954        68.757            -326.029    -50.27    -2.821    4        100.0\n",
      "            v3|    538.712        68.045            408.379    670.578    7.917    1        95.126\n",
      "            v4|    302.388        66.225            166.441    424.102    4.566    3        98.168\n",
      "            v5|    -140.722        167.302            -611.971    58.538    -0.841    6        45.01\n",
      "            v6|    4.097        122.219            -200.588    396.782    0.034    7        62.943\n",
      "            v7|    -173.298        121.283            -391.73    27.684    -1.429    5        43.909\n",
      "            v8|    51.075        103.761            -114.607    314.754    0.492    7        48.748\n",
      "            v9|    534.36        99.098            351.205    753.415    5.392    1        61.177\n",
      "            v10|    30.217        49.082            -36.692    147.247    0.616    7        88.776\n",
      "            cons|    152.157        2.556            147.083    157.179        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'gaussian', 'horseshoe+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/gaussian_horseshoe+.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace model with ridge prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear ridge regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =62.073\n",
      "R-Squared       =0.512\n",
      "WAIC            =2425.955 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -25.633        59.051            -148.229    81.805    -0.434    10        64.245\n",
      "            v2|    -301.471        59.505            -420.812    -188.937    -5.066    4        66.267\n",
      "            v3|    473.344        68.598            332.354    614.994    6.9    2        72.281\n",
      "            v4|    376.237        66.431            244.245    505.283    5.664    3        64.243\n",
      "            v5|    -209.831        224.089            -680.928    177.998    -0.936    5        98.073\n",
      "            v6|    -37.992        193.543            -374.588    369.991    -0.196    8        97.459\n",
      "            v7|    -165.863        132.991            -417.673    117.205    -1.247    5        91.874\n",
      "            v8|    143.012        150.422            -150.551    430.448    0.951    5        84.37\n",
      "            v9|    546.622        109.357            348.626    787.406    4.998    1        76.077\n",
      "            v10|    62.552        68.809            -72.917    194.16    0.909    8        64.622\n",
      "            cons|    150.804        2.852            145.323    156.153        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'laplace', 'ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/laplace_ridge.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace model with Lasso prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear lasso regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =61.943\n",
      "R-Squared       =0.513\n",
      "WAIC            =2425.464 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -21.557        54.482            -138.932    81.9    -0.396    9        81.402\n",
      "            v2|    -290.029        62.808            -409.183    -169.562    -4.618    4        72.573\n",
      "            v3|    478.17        69.507            341.612    620.739    6.879    1        72.797\n",
      "            v4|    368.158        68.594            240.786    497.233    5.367    3        62.007\n",
      "            v5|    -221.251        201.417            -676.943    100.612    -1.098    5        85.698\n",
      "            v6|    -10.586        160.369            -300.764    384.092    -0.066    8        89.208\n",
      "            v7|    -162.494        121.709            -380.983    92.76    -1.335    6        76.443\n",
      "            v8|    114.787        136.718            -136.22    386.076    0.84    6        76.589\n",
      "            v9|    572.782        106.668            385.619    817.298    5.37    1        74.194\n",
      "            v10|    51.198        63.29            -70.792    170.882    0.809    9        77.976\n",
      "            cons|    150.809        2.716            145.758    156.32        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'laplace', 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/laplace_lasso.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lapalce model with horseshoe prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear horseshoe regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =61.971\n",
      "R-Squared       =0.512\n",
      "WAIC            =2425.505 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -16.782        44.877            -115.483    63.747    -0.374    10        60.13\n",
      "            v2|    -284.198        66.356            -409.867    -153.04    -4.283    4        58.792\n",
      "            v3|    489.036        69.274            348.245    626.715    7.059    2        64.762\n",
      "            v4|    361.924        69.371            229.207    492.418    5.217    3        52.088\n",
      "            v5|    -202.689        186.085            -633.918    82.478    -1.089    5        53.76\n",
      "            v6|    -17.913        138.862            -287.101    335.386    -0.129    7        71.487\n",
      "            v7|    -174.103        123.854            -401.523    44.278    -1.406    5        49.683\n",
      "            v8|    80.909        127.066            -124.438    369.814    0.637    7        53.833\n",
      "            v9|    588.623        105.467            376.854    785.021    5.581    1        61.34\n",
      "            v10|    31.172        58.065            -70.945    171.494    0.537    7        67.029\n",
      "            cons|    150.538        2.846            145.509    156.276        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'laplace', 'horseshoe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/laplace_horseshoe.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace model with horseshoe+ prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear horseshoe+ regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =61.918\n",
      "R-Squared       =0.512\n",
      "WAIC            =2424.741 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -11.702        37.911            -103.66    57.512    -0.309    9        59.345\n",
      "            v2|    -288.598        63.528            -417.441    -156.002    -4.543    4        60.653\n",
      "            v3|    484.961        72.141            348.603    629.713    6.722    2        73.885\n",
      "            v4|    362.819        67.65            224.906    489.488    5.363    3        57.934\n",
      "            v5|    -204.696        199.999            -707.471    59.594    -1.023    5        36.135\n",
      "            v6|    1.758        147.089            -251.724    408.92    0.012    7        55.042\n",
      "            v7|    -179.41        124.961            -396.327    43.814    -1.436    5        30.346\n",
      "            v8|    68.553        126.001            -123.853    384.165    0.544    7        30.912\n",
      "            v9|    598.055        106.635            410.831    839.264    5.608    1        42.887\n",
      "            v10|    17.892        42.16            -58.031    127.161    0.424    9        65.061\n",
      "            cons|    150.769        2.735            145.303    155.911        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 'laplace', 'horseshoe+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/laplace_horseshoe+.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T model with ridge prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear ridge regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =60.858\n",
      "R-Squared       =0.515\n",
      "WAIC            =2409.255 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -18.01        58.801            -136.799    102.86    -0.306    10        89.367\n",
      "            v2|    -265.852        62.34            -389.318    -141.207    -4.265    4        100.0\n",
      "            v3|    518.63        66.285            383.677    652.052    7.824    1        100.0\n",
      "            v4|    330.423        61.327            204.051    452.531    5.388    3        100.0\n",
      "            v5|    -200.239        213.756            -619.112    214.31    -0.937    5        97.013\n",
      "            v6|    -13.734        182.087            -349.768    369.756    -0.075    6        100.0\n",
      "            v7|    -165.731        126.867            -430.979    79.404    -1.306    6        100.0\n",
      "            v8|    111.651        136.432            -161.238    383.726    0.818    6        100.0\n",
      "            v9|    549.905        105.864            362.904    772.128    5.194    1        93.892\n",
      "            v10|    57.032        66.822            -72.492    185.607    0.853    9        92.743\n",
      "            cons|    151.421        2.567            146.479    156.465        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 't', 'ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/t_ridge.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T model with Lasso prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear lasso regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =60.825\n",
      "R-Squared       =0.515\n",
      "WAIC            =2408.954 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -16.166        50.193            -118.36    81.142    -0.322    10        100.0\n",
      "            v2|    -254.827        63.449            -383.299    -132.64    -4.016    4        92.795\n",
      "            v3|    526.419        68.858            386.19    659.939    7.645    1        89.618\n",
      "            v4|    325.472        63.681            196.774    452.909    5.111    3        92.649\n",
      "            v5|    -201.829        194.41            -669.021    123.985    -1.038    5        85.993\n",
      "            v6|    -3.105        158.702            -282.827    382.312    -0.02    7        93.284\n",
      "            v7|    -160.436        119.489            -384.666    79.229    -1.343    6        81.982\n",
      "            v8|    89.775        125.849            -134.742    350.882    0.713    7        80.559\n",
      "            v9|    568.713        102.002            369.164    780.11    5.576    1        95.447\n",
      "            v10|    46.114        61.614            -69.039    170.388    0.748    7        100.0\n",
      "            cons|    151.488        2.706            146.496    156.779        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 't', 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/t_lasso.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T model with horseshoe prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear horseshoe regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =60.919\n",
      "R-Squared       =0.514\n",
      "WAIC            =2408.963 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -10.697        43.323            -105.893    74.096    -0.247    10        100.0\n",
      "            v2|    -244.022        64.374            -374.216    -114.3    -3.791    4        100.0\n",
      "            v3|    537.912        71.688            397.403    682.782    7.504    1        89.482\n",
      "            v4|    315.411        69.273            182.085    453.3    4.553    3        84.376\n",
      "            v5|    -191.206        184.26            -612.327    61.572    -1.038    5        49.259\n",
      "            v6|    3.417        142.758            -248.443    350.469    0.024    7        58.587\n",
      "            v7|    -164.158        117.81            -391.019    36.433    -1.393    5        50.071\n",
      "            v8|    63.42        109.239            -115.245    313.711    0.581    7        64.699\n",
      "            v9|    585.301        104.778            386.926    811.353    5.586    1        66.551\n",
      "            v10|    28.386        53.566            -67.275    155.094    0.53    7        100.0\n",
      "            cons|    151.35        2.694            146.146    156.942        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 't', 'horseshoe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/t_horseshoe.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T model with horseshoe+ prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear horseshoe+ regression \n",
      "\n",
      "\n",
      "Number of obs   =442\n",
      "Number of vars  =10 \n",
      "\n",
      "MCMC Samples    =1000\n",
      "MCMC Burnin     =1000\n",
      "MCMC Thining    =5 \n",
      "\n",
      "Root MSE        =60.861\n",
      "R-Squared       =0.514\n",
      "WAIC            =2408.798 \n",
      "\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "  Parameter      mean(Coef)    std(Coef)        [95% Cred. Interval]      tStat    Rank       ESS\n",
      "------------- + -----------------------------------------------------------------------------------\n",
      "            v1|    -7.778        35.431            -89.172    60.803    -0.22    10        96.193\n",
      "            v2|    -245.847        65.034            -371.946    -120.287    -3.78    4        68.981\n",
      "            v3|    538.858        69.023            400.754    677.66    7.807    1        100.0\n",
      "            v4|    317.417        67.558            177.272    445.018    4.698    3        100.0\n",
      "            v5|    -183.812        188.728            -647.349    52.295    -0.974    5        47.761\n",
      "            v6|    4.664        140.286            -240.372    407.311    0.033    7        61.099\n",
      "            v7|    -169.942        121.121            -386.869    28.519    -1.403    5        47.619\n",
      "            v8|    55.113        106.535            -98.501    311.886    0.517    7        58.657\n",
      "            v9|    590.405        105.269            405.549    809.542    5.609    1        62.808\n",
      "            v10|    17.406        42.163            -55.415    121.698    0.413    7        86.87\n",
      "            cons|    151.482        2.594            146.116    156.617        .    None        .\n"
     ]
    }
   ],
   "source": [
    "beta, beta0, retval = bayesreg(X.copy(), y.copy(), 't', 'horseshoe+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corresponding Matlab result\n",
    "<img src=\"img/t_horseshoe+.png\" width='600px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for future work\n",
    "\n",
    "Please ignore for now. Code below does not have an effect on the current procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sample_delta2_hs(b, sigma2, tau2, lambda2, rho, delta2prod, delta2, groups, nGroups, GroupSizes):\n",
    "    delta2prod = delta2prod/delta2[groups]\n",
    "    K = b**2/lambda2/delta2prod\n",
    "    \n",
    "    scale = np.zeros(shape = (1, nGroups))\n",
    "    for i in range(nGroups):\n",
    "        ix = (groups == i)\n",
    "        scale[i] = 1/rho[i] + 1/2/tau2/sigma2 * np.sum(K[ix], axis=0)\n",
    "    delta2[:-1] = scale/np.random.gamma(shape = 1, scale=(GroupSizes.T+1)/2)\n",
    "    \n",
    "    delta2prod = delta2prod * delta2[groups]\n",
    "    return delta2, delta2prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sample_delta2_lasso(b, sigma2, tau2, lambda2, delta2prod, delta2, groups, nGroups, GroupSizes):\n",
    "    delta2prod = delta2prod/delta2[groups]\n",
    "    K = b**2/lambda2/delta2prod\n",
    "    \n",
    "    for i in range(nGroups):\n",
    "        ix = (groups == i)\n",
    "        \n",
    "        gig_p = GroupSizes[i]/2 - 1\n",
    "        gig_a = 1/tau2/sigma2 * np.sum(K[ix], axis=0)\n",
    "        gig_b = 2\n",
    "        \n",
    "        delta2[i] = gigrnd(gig_p, gig_a, gig_b, 1)\n",
    "    delta2prod = delta2prod * delta2[groups]\n",
    "    return delta2, delta2prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sample_nu_hsplus(lambda2, phi2):\n",
    "    scale = 1/phi2 + 1/lambda2\n",
    "    nu = 1/exprnd_fast(1/scale)\n",
    "    return nu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sample_phi2_hsplus(mu, zeta):\n",
    "    scale = 1/nu + 1/zeta\n",
    "    phi2 = 1/exprnd_fast(1/scale)\n",
    "    return phi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sample_zeta_hsplus(phi2):\n",
    "    scale = 1 + 1/phi2\n",
    "    zeta = 1/exprnd_fast(1/scale)\n",
    "    return zeta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
